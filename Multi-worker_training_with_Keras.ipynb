{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7f100f-a662-47e3-ade2-864ed249231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf2ec42-3ed9-4b86-9f74-d0472eb81cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94481f83-d8cc-42a1-8385-2f24e84c2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.pop('TF_CONFIG', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941baf1b-bbc0-4310-bb60-0946fa18b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.' not in sys.path:\n",
    "  sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0215931-ddde-49ff-a934-36270bdfa8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:14:16.608024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:14:16.608048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:14:16.608947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:14:16.613640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:14:17.185998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b96554-4b43-4d0b-8fd6-ffa8472b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnist_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnist_setup.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def mnist_dataset(batch_size):\n",
    "  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "  # The `x` arrays are in uint8 and have values in the [0, 255] range.\n",
    "  # You need to convert them to float32 with values in the [0, 1] range.\n",
    "  x_train = x_train / np.float32(255)\n",
    "  y_train = y_train.astype(np.int64)\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n",
    "  return train_dataset\n",
    "\n",
    "def build_and_compile_cnn_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0cf100-cc5d-4696-a55c-c9867e826571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:14:54.790252: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-27 15:14:54.790275: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: xy\n",
      "2025-11-27 15:14:54.790279: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: xy\n",
      "2025-11-27 15:14:54.790350: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 580.95.5\n",
      "2025-11-27 15:14:54.790366: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 580.95.5\n",
      "2025-11-27 15:14:54.790370: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 580.95.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 1s 4ms/step - loss: 2.2774 - accuracy: 0.2065\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.2108 - accuracy: 0.3806\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 2.1305 - accuracy: 0.5721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79e46be920>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist_setup\n",
    "\n",
    "batch_size = 64\n",
    "single_worker_dataset = mnist_setup.mnist_dataset(batch_size)\n",
    "single_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed516420-3656-4809-8c28-6d4cd23056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config = {\n",
    "    'cluster': {\n",
    "        'worker': ['localhost:12345', 'localhost:23456']\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69dae64-a2bd-4d55-8392-59f0aafdab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f72ac5-e52e-46c5-8b32-c7e1740a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GREETINGS'] = 'Hello TensorFlow!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36bfc1d-f304-4929-9d1f-243e9621e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello TensorFlow!\n"
     ]
    }
   ],
   "source": [
    "!echo ${GREETINGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5c552f-2fb7-4ac9-a826-46b1391109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88018444-871b-4949-b9e0-b1b291170f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Model building/compiling need to be within `strategy.scope()`.\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937c4a4e-0755-45c9-a414-fb99a99e070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import mnist_setup\n",
    "\n",
    "per_worker_batch_size = 64\n",
    "tf_config = json.loads(os.environ['TF_CONFIG'])\n",
    "num_workers = len(tf_config['cluster']['worker'])\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "global_batch_size = per_worker_batch_size * num_workers\n",
    "multi_worker_dataset = mnist_setup.mnist_dataset(global_batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "  # Model building/compiling need to be within `strategy.scope()`.\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "\n",
    "\n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a0abc6-2dc9-4f78-9435-d2a91643b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py  mnist_setup.py\n"
     ]
    }
   ],
   "source": [
    "!ls *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9184e359-c5f7-421e-ab80-29cae6c3a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a03de98-93fd-4aa3-8480-2ae245b021ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "# first kill any previous runs\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8b36889-9195-4551-bb92-afcc34f076cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py &> job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0290a7f7-3505-42cd-b8f4-270ce62dd820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "754f51c9-fd3e-4fe9-9b48-2707434a318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:19:23.057559: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:19:23.057608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:19:23.058465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:19:23.062730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:19:23.622599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-11-27 15:19:24.234213: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-27 15:19:24.234253: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: xy\n",
      "2025-11-27 15:19:24.234263: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: xy\n",
      "2025-11-27 15:19:24.234315: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 580.95.5\n",
      "2025-11-27 15:19:24.234335: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 580.95.5\n",
      "2025-11-27 15:19:24.234343: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 580.95.5\n",
      "E1127 15:19:24.237068784   18062 server_chttp2.cc:40]        {\"created\":\"@1764253164.237041904\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1764253164.237040494\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1764253164.237029004\",\"description\":\"Unable to configure socket\",\"fd\":10,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1764253164.237025724\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1764253164.237040174\",\"description\":\"Unable to configure socket\",\"fd\":10,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1764253164.237037784\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\n",
      "2025-11-27 15:19:24.237096: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server\n",
      "2025-11-27 15:19:24.237173: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:783] Could not start gRPC server\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xy/Desktop/ml/TF/main.py\", line 12, in <module>\n",
      "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 186, in __init__\n",
      "    CollectiveAllReduceExtended(\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 339, in __init__\n",
      "    self._initialize_strategy(self._cluster_resolver, devices=devices)\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 358, in _initialize_strategy\n",
      "    self._initialize_multi_worker(cluster_resolver)\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 530, in _initialize_multi_worker\n",
      "    context.context().ensure_initialized()\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 617, in ensure_initialized\n",
      "    pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n"
     ]
    }
   ],
   "source": [
    "!cat job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af3ba23-d487-47a1-8f2e-88b8e918a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config['task']['index'] = 1\n",
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95cda7f8-bda9-4e67-88cb-2d138ab981ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:19:46.652786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:19:46.652812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:19:46.653654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:19:46.657968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:19:47.223866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-11-27 15:19:47.831175: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-27 15:19:47.831196: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: xy\n",
      "2025-11-27 15:19:47.831202: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: xy\n",
      "2025-11-27 15:19:47.831259: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 580.95.5\n",
      "2025-11-27 15:19:47.831278: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 580.95.5\n",
      "2025-11-27 15:19:47.831283: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 580.95.5\n",
      "2025-11-27 15:19:47.836756: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:23456\n",
      "2025-11-27 15:19:47.837633: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:304] Coordination agent has successfully connected.\n",
      "2025-11-27 15:19:53.367447: E external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:770] Coordination agent is set to ERROR: UNAVAILABLE: failed to connect to all addresses\n",
      "Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.CoordinationService/Heartbeat:\n",
      ":{\"created\":\"@1764253193.367400499\",\"description\":\"Failed to pick subchannel\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3941,\"referenced_errors\":[{\"created\":\"@1764253193.362965170\",\"description\":\"failed to connect to all addresses\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":393,\"grpc_status\":14}]}\n",
      "2025-11-27 15:19:53.367484: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNAVAILABLE: failed to connect to all addresses\n",
      "Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.CoordinationService/Heartbeat:\n",
      ":{\"created\":\"@1764253193.367400499\",\"description\":\"Failed to pick subchannel\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3941,\"referenced_errors\":[{\"created\":\"@1764253193.362965170\",\"description\":\"failed to connect to all addresses\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":393,\"grpc_status\":14}]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xy/Desktop/ml/TF/main.py\", line 19, in <module>\n",
      "    multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
      "  File \"/home/xy/Desktop/ml/TF/mnist_setup.py\", line 17, in build_and_compile_cnn_model\n",
      "    model = tf.keras.Sequential([\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.UnavailableError: {{function_node __wrapped__CollectiveBcastRecv_device_/job:worker/replica:0/task:1/device:CPU:0}} Collective ops is aborted by: failed to connect to all addresses\n",
      "Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.CoordinationService/Heartbeat:\n",
      ":{\"created\":\"@1764253193.367400499\",\"description\":\"Failed to pick subchannel\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3941,\"referenced_errors\":[{\"created\":\"@1764253193.362965170\",\"description\":\"failed to connect to all addresses\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":393,\"grpc_status\":14}]}\n",
      "The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastRecv]\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed31a0c6-019d-45a3-a191-f46a1a76b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:19:23.057559: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:19:23.057608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:19:23.058465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:19:23.062730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:19:23.622599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-11-27 15:19:24.234213: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-27 15:19:24.234253: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: xy\n",
      "2025-11-27 15:19:24.234263: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: xy\n",
      "2025-11-27 15:19:24.234315: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 580.95.5\n",
      "2025-11-27 15:19:24.234335: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 580.95.5\n",
      "2025-11-27 15:19:24.234343: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 580.95.5\n",
      "E1127 15:19:24.237068784   18062 server_chttp2.cc:40]        {\"created\":\"@1764253164.237041904\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1764253164.237040494\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1764253164.237029004\",\"descrip2025-11-27 15:19:47.837524: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 5671159828748267439\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xy/Desktop/ml/TF/main.py\", line 12, in <module>\n",
      "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 186, in __init__\n",
      "    CollectiveAllReduceExtended(\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 339, in __init__\n",
      "    self._initialize_strategy(self._cluster_resolver, devices=devices)\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 358, in _initialize_strategy\n",
      "    self._initialize_multi_worker(cluster_resolver)\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 530, in _initialize_multi_worker\n",
      "    context.context().ensure_initialized()\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 617, in ensure_initialized\n",
      "    pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)\n",
      "KeyboardInterrupt\n",
      "2025-11-27 15:19:47.903060: W tensorflow/core/common_runtime/eager/context.cc:628] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\n",
      "2025-11-27 15:19:47.903102: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:473] Coordination agent has initiated Shutdown().\n",
      "2025-11-27 15:19:47.903279: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:658] /job:worker/replica:0/task:0 has disconnected from coordination service.\n",
      "2025-11-27 15:19:47.903379: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:492] Coordination agent has successfully shut down.\n",
      "ized()\n",
      "  File \"/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 617, in ensure_initialized\n",
      "    pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n"
     ]
    }
   ],
   "source": [
    "!cat job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfd6dc26-28ca-4077-8a50-2fbb853deeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "# Delete the `TF_CONFIG`, and kill any background tasks so they don't affect the next section.\n",
    "os.environ.pop('TF_CONFIG', None)\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ec3cab6-dc86-44fc-be98-b2ecb0af5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "\n",
    "global_batch_size = 64\n",
    "multi_worker_dataset = mnist_setup.mnist_dataset(batch_size=64)\n",
    "dataset_no_auto_shard = multi_worker_dataset.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c575da7d-8739-4a84-8eae-012f10c4effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a2824d3-f480-4a3e-9c84-87c655751667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/tmp/keras-model'\n",
    "\n",
    "def _is_chief(task_type, task_id):\n",
    "  # Note: there are two possible `TF_CONFIG` configurations.\n",
    "  #   1) In addition to `worker` tasks, a `chief` task type is use;\n",
    "  #      in this case, this function should be modified to\n",
    "  #      `return task_type == 'chief'`.\n",
    "  #   2) Only `worker` task type is used; in this case, worker 0 is\n",
    "  #      regarded as the chief. The implementation demonstrated here\n",
    "  #      is for this case.\n",
    "  # For the purpose of this Colab section, the `task_type` is `None` case\n",
    "  # is added because it is effectively run with only a single worker.\n",
    "  return (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "def _get_temp_dir(dirpath, task_id):\n",
    "  base_dirpath = 'workertemp_' + str(task_id)\n",
    "  temp_dir = os.path.join(dirpath, base_dirpath)\n",
    "  tf.io.gfile.makedirs(temp_dir)\n",
    "  return temp_dir\n",
    "\n",
    "def write_filepath(filepath, task_type, task_id):\n",
    "  dirpath = os.path.dirname(filepath)\n",
    "  base = os.path.basename(filepath)\n",
    "  if not _is_chief(task_type, task_id):\n",
    "    dirpath = _get_temp_dir(dirpath, task_id)\n",
    "  return os.path.join(dirpath, base)\n",
    "\n",
    "task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                      strategy.cluster_resolver.task_id)\n",
    "write_model_path = write_filepath(model_path, task_type, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e286ec6e-6ed3-4bf6-9fb2-12558f25f68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/keras-model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/keras-model/assets\n"
     ]
    }
   ],
   "source": [
    "multi_worker_model.save(write_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "affe8b93-1689-418f-ae01-da674694a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _is_chief(task_type, task_id):\n",
    "  tf.io.gfile.rmtree(os.path.dirname(write_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b31897b4-2ca2-4aa3-ade0-0715f33601ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 2.2857 - accuracy: 0.0023\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 2.2697 - accuracy: 0.0086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7a7728c970>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Now that the model is restored, and can continue with the training.\n",
    "loaded_model.fit(single_worker_dataset, epochs=2, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e72c4edb-9a39-4582-9f10-7f2ccc06a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/tmp/ckpt'\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=multi_worker_model)\n",
    "write_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id)\n",
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fa13d04-1f81-4fae-af9f-e2a1c14afed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager.save()\n",
    "if not _is_chief(task_type, task_id):\n",
    "  tf.io.gfile.rmtree(write_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98b69a5c-5869-4150-80a6-f565d7a3bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:21:25.463301: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step - loss: 2.2834 - accuracy: 7.8125e-04\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 2.2729 - accuracy: 0.0039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79f5946410>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint.restore(latest_checkpoint)\n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=2, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41b7b261-0ff2-4e68-b747-859e42dff596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 2.2928 - accuracy: 0.1368\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.2333 - accuracy: 0.3167\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.1680 - accuracy: 0.5007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79f08ab610>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-worker training with `MultiWorkerMirroredStrategy`\n",
    "# and the `BackupAndRestore` callback. The training state \n",
    "# is backed up at epoch boundaries by default.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e97d6141-2c2e-40e9-b04a-fe60a85f3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 2.2825 - accuracy: 0.1533\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.2429 - accuracy: 0.2478\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.2047 - accuracy: 0.3509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79f79b7e80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training state is backed up at epoch boundaries because `save_freq` is\n",
    "# set to `epoch`.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2784e3c-1fe0-415c-9516-6f15c86f836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.2921 - accuracy: 0.1187\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.2441 - accuracy: 0.2484\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.1954 - accuracy: 0.4306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f79f609dc30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training state is backed up at every 30 steps because `save_freq` is set\n",
    "# to an integer value of `30`.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup', save_freq=30)]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8e48d-89f2-4805-b4d4-5d7e8700173d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da58a2-8ef4-44fe-9bbe-233e0e04b8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8d7fb-eaa9-4aa1-a7ba-346a4c74252e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec4f39-e5d9-4626-81ca-b6761f5e4ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab2e7c-a0fa-4ebd-ac7d-e5bb2bf4a033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431d57f-d38f-44b9-8e65-cc82cba41768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb7f26-d7f3-4126-acf0-e6d97425adba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c3bb9-560a-4712-9371-2adb3595427f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff5595-11e0-492a-8a7b-07b3ea643b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767ce09-26d4-4191-9aaa-f26aec14d9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ed999-11a2-4bef-9816-eba514226b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fee172-5da6-4933-b8ba-3bda7bf2f8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4921eca-037b-44d4-b78d-19aac39b5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a1418-6941-47a1-ac46-3b37eeafa825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95719-1918-4f98-a09c-9cc375dcafc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
