{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4100d870-3f5b-4a4d-bf69-2f1f7f26955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513a8c7b-0cb3-4ee4-9b4f-d22268046033",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cce6a6-131f-4dfa-b18e-43c68d014f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.pop('TF_CONFIG', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74305b93-1fdc-4c0c-9471-355b816a0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.' not in sys.path:\n",
    "  sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a201693-5a3b-473d-b46d-3ccd9101c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:24:09.084611: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:24:09.084635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:24:09.085476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:24:09.090290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:24:09.652502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2328fdcb-674a-46ee-a82b-b830ea650318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnist.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def mnist_dataset(batch_size):\n",
    "  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "  # The `x` arrays are in uint8 and have values in the range [0, 255].\n",
    "  # You need to convert them to float32 with values in the range [0, 1]\n",
    "  x_train = x_train / np.float32(255)\n",
    "  y_train = y_train.astype(np.int64)\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, y_train)).shuffle(60000)\n",
    "  return train_dataset\n",
    "\n",
    "def dataset_fn(global_batch_size, input_context):\n",
    "  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "  dataset = mnist_dataset(batch_size)\n",
    "  dataset = dataset.shard(input_context.num_input_pipelines,\n",
    "                          input_context.input_pipeline_id)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "def build_cnn_model():\n",
    "  regularizer = tf.keras.regularizers.L2(1e-5)\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.Input(shape=(28, 28)),\n",
    "      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.Conv2D(32, 3,\n",
    "                             activation='relu',\n",
    "                             kernel_regularizer=regularizer),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(128,\n",
    "                            activation='relu',\n",
    "                            kernel_regularizer=regularizer),\n",
    "      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15df1b7d-6fd9-45cd-8390-450d8cf61b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config = {\n",
    "    'cluster': {\n",
    "        'worker': ['localhost:12345', 'localhost:23456']\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7e7d21-3419-4759-a087-5f9263716034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168d4505-ad8c-4afe-beab-8bc937235d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GREETINGS'] = 'Hello TensorFlow!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43845774-cce6-47b7-bdcd-a8420e469804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello TensorFlow!\n"
     ]
    }
   ],
   "source": [
    "!echo ${GREETINGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bc7569-8b7b-4c26-9997-50399aee4f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:29:19.061949: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-27 15:29:19.061969: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: xy\n",
      "2025-11-27 15:29:19.061973: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: xy\n",
      "2025-11-27 15:29:19.062065: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 580.95.5\n",
      "2025-11-27 15:29:19.062085: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 580.95.5\n",
      "2025-11-27 15:29:19.062089: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 580.95.5\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0419ec-de24-4e41-ae6d-c8003c09ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "with strategy.scope():\n",
    "  # Model building needs to be within `strategy.scope()`.\n",
    "  multi_worker_model = mnist.build_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d41ded-8b6d-4500-b413-fd01586ff367",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_worker_batch_size = 64\n",
    "num_workers = len(tf_config['cluster']['worker'])\n",
    "global_batch_size = per_worker_batch_size * num_workers\n",
    "\n",
    "with strategy.scope():\n",
    "  multi_worker_dataset = strategy.distribute_datasets_from_function(\n",
    "      lambda input_context: mnist.dataset_fn(global_batch_size, input_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa8621f-b795-4a8f-8c46-e80d1a44bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # The creation of optimizer and train_accuracy needs to be in\n",
    "  # `strategy.scope()` as well, since they create variables.\n",
    "  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29754f57-67fb-41e3-8fce-d217b164ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(iterator):\n",
    "  \"\"\"Training step function.\"\"\"\n",
    "\n",
    "  def step_fn(inputs):\n",
    "    \"\"\"Per-Replica step function.\"\"\"\n",
    "    x, y = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = multi_worker_model(x, training=True)\n",
    "      per_example_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "          from_logits=True,\n",
    "          reduction=tf.keras.losses.Reduction.NONE)(y, predictions)\n",
    "      loss = tf.nn.compute_average_loss(per_example_loss)\n",
    "      model_losses = multi_worker_model.losses\n",
    "      if model_losses:\n",
    "        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "\n",
    "    grads = tape.gradient(loss, multi_worker_model.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads, multi_worker_model.trainable_variables))\n",
    "    train_accuracy.update_state(y, predictions)\n",
    "    return loss\n",
    "\n",
    "  per_replica_losses = strategy.run(step_fn, args=(next(iterator),))\n",
    "  return strategy.reduce(\n",
    "      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfffa2d0-f5f8-4821-a640-5e32b11e5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import util\n",
    "checkpoint_dir = os.path.join(util.get_temp_dir(), 'ckpt')\n",
    "\n",
    "def _is_chief(task_type, task_id, cluster_spec):\n",
    "  return (task_type is None\n",
    "          or task_type == 'chief'\n",
    "          or (task_type == 'worker'\n",
    "              and task_id == 0\n",
    "              and \"chief\" not in cluster_spec.as_dict()))\n",
    "\n",
    "def _get_temp_dir(dirpath, task_id):\n",
    "  base_dirpath = 'workertemp_' + str(task_id)\n",
    "  temp_dir = os.path.join(dirpath, base_dirpath)\n",
    "  tf.io.gfile.makedirs(temp_dir)\n",
    "  return temp_dir\n",
    "\n",
    "def write_filepath(filepath, task_type, task_id, cluster_spec):\n",
    "  dirpath = os.path.dirname(filepath)\n",
    "  base = os.path.basename(filepath)\n",
    "  if not _is_chief(task_type, task_id, cluster_spec):\n",
    "    dirpath = _get_temp_dir(dirpath, task_id)\n",
    "  return os.path.join(dirpath, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f76930f-083b-4941-bd27-076f96c83514",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = tf.Variable(\n",
    "    initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\n",
    "step_in_epoch = tf.Variable(\n",
    "    initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n",
    "    name='step_in_epoch')\n",
    "task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                      strategy.cluster_resolver.task_id)\n",
    "# Normally, you don't need to manually instantiate a `ClusterSpec`, but in this\n",
    "# illustrative example you did not set `'TF_CONFIG'` before initializing the\n",
    "# strategy. Check out the next section for \"real-world\" usage.\n",
    "cluster_spec = tf.train.ClusterSpec(tf_config['cluster'])\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    model=multi_worker_model, epoch=epoch, step_in_epoch=step_in_epoch)\n",
    "\n",
    "write_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id,\n",
    "                                      cluster_spec)\n",
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfac37e1-8cd8-4632-b555-e838672aeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "  checkpoint.restore(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b9417c-5b6d-4ef8-9a33-212d60908b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:31:43.230083: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, accuracy: 0.814062, train_loss: 0.595369.\n",
      "Epoch: 1, accuracy: 0.925223, train_loss: 0.258510.\n",
      "Epoch: 2, accuracy: 0.948772, train_loss: 0.176717.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_steps_per_epoch = 70\n",
    "\n",
    "while epoch.numpy() < num_epochs:\n",
    "  iterator = iter(multi_worker_dataset)\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "\n",
    "  while step_in_epoch.numpy() < num_steps_per_epoch:\n",
    "    total_loss += train_step(iterator)\n",
    "    num_batches += 1\n",
    "    step_in_epoch.assign_add(1)\n",
    "\n",
    "  train_loss = total_loss / num_batches\n",
    "  print('Epoch: %d, accuracy: %f, train_loss: %f.'\n",
    "                %(epoch.numpy(), train_accuracy.result(), train_loss))\n",
    "\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # Once the `CheckpointManager` is set up, you're now ready to save, and remove\n",
    "  # the checkpoints non-chief workers saved.\n",
    "  checkpoint_manager.save()\n",
    "  if not _is_chief(task_type, task_id, cluster_spec):\n",
    "    tf.io.gfile.rmtree(write_checkpoint_dir)\n",
    "\n",
    "  epoch.assign_add(1)\n",
    "  step_in_epoch.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ca74d3-648e-4d24-b88f-4932210df621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py  mnist.py  mnist_setup.py\n"
     ]
    }
   ],
   "source": [
    "!ls *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7908fff9-466d-4906-9d89-ba38e9cb54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b2647de-de86-4c1b-8c0b-c9c396d3052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "# first kill any previous runs\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9c349f2-39d9-41b4-bc7d-b8c635327c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py &> job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5865e003-babe-44df-87e5-c54f451e9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ace71218-841b-44df-a6de-ad44f6b3e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/xy/Desktop/ml/TF/main.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c53a177e-a7da-4613-a996-9c9b235a3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config['task']['index'] = 1\n",
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "370b889a-ec5c-4068-b6ea-268947200522",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f8e743c-fde8-4a2e-85a1-f88609ff2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/xy/Desktop/ml/TF/main.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat job_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "724b5384-fc18-49e3-9e00-c8172a6ec01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "# Delete the `'TF_CONFIG'`, and kill any background tasks so they don't affect the next section.\n",
    "os.environ.pop('TF_CONFIG', None)\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2c580-137e-43a1-8010-b1cb79c5607e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595a63c-24bf-44d2-9c41-927f4e206eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b82f58-e2cc-43cf-b1b4-cae9672e6e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca5fab-dcdf-4f19-bc74-93934532c155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6104f-6325-4a1c-a486-8452e2468d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafb02b-6e3f-434b-8984-583ce3e2a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898410d-84bd-48d0-8f05-e399de3a0d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9d88c-fc3b-4e79-bbb9-048fc2fa54ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
