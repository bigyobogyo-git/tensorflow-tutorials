{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb0ff98-eb87-4b69-9e26-a415530c8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:39:38.338475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 15:39:38.338502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 15:39:38.339385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 15:39:38.343999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-11-27 15:39:38.968331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import portpicker\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2693c2f0-7b92-4622-85d0-42b4ef1a8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:39:40.468592: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.501969: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.504573: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.660409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.661446: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.662381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.663172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:worker/replica:0/task:0/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:40.667462: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:42511\n",
      "2025-11-27 15:39:40.668032: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.668968: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.669790: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.670636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.671650: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.672435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:worker/replica:0/task:1/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:40.680066: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:37261\n",
      "2025-11-27 15:39:40.680264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.681156: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.681983: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.682842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.683676: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.684498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:worker/replica:0/task:2/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:40.693260: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:39073\n",
      "2025-11-27 15:39:40.693814: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.694765: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.695664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.696591: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.697576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.698474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:ps/replica:0/task:0/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:40.709104: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:34697\n",
      "2025-11-27 15:39:40.709272: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.710224: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.711211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.712263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.713147: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:40.713947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:ps/replica:0/task:1/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:40.726735: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:35355\n"
     ]
    }
   ],
   "source": [
    "def create_in_process_cluster(num_workers, num_ps):\n",
    "  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\n",
    "  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\n",
    "  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n",
    "\n",
    "  cluster_dict = {}\n",
    "  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\n",
    "  if num_ps > 0:\n",
    "    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n",
    "\n",
    "  cluster_spec = tf.train.ClusterSpec(cluster_dict)\n",
    "\n",
    "  # Workers need some inter_ops threads to work properly.\n",
    "  worker_config = tf.compat.v1.ConfigProto()\n",
    "  if multiprocessing.cpu_count() < num_workers + 1:\n",
    "    worker_config.inter_op_parallelism_threads = num_workers + 1\n",
    "\n",
    "  for i in range(num_workers):\n",
    "    tf.distribute.Server(\n",
    "        cluster_spec,\n",
    "        job_name=\"worker\",\n",
    "        task_index=i,\n",
    "        config=worker_config,\n",
    "        protocol=\"grpc\")\n",
    "\n",
    "  for i in range(num_ps):\n",
    "    tf.distribute.Server(\n",
    "        cluster_spec,\n",
    "        job_name=\"ps\",\n",
    "        task_index=i,\n",
    "        protocol=\"grpc\")\n",
    "\n",
    "  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n",
    "      cluster_spec, rpc_layer=\"grpc\")\n",
    "  return cluster_resolver\n",
    "\n",
    "# Set the environment variable to allow reporting worker and ps failure to the\n",
    "# coordinator. This is a workaround and won't be necessary in the future.\n",
    "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n",
    "\n",
    "NUM_WORKERS = 3\n",
    "NUM_PS = 2\n",
    "cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351a437e-27ae-47d2-b9e8-8ceeb11436f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:`tf.distribute.experimental.ParameterServerStrategy` is initialized with cluster_spec: ClusterSpec({'ps': ['localhost:34697', 'localhost:35355'], 'worker': ['localhost:42511', 'localhost:37261', 'localhost:39073']})\n",
      "INFO:tensorflow:ParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'ps': ['localhost:34697', 'localhost:35355'], 'worker': ['localhost:42511', 'localhost:37261', 'localhost:39073']})\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:chief/replica:0/task:0/device:GPU:0'], variable_device = '/job:chief/replica:0/task:0/device:GPU:0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:39:55.281879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.282859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.283689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.284745: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.285565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.286377: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.287234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.288189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.288972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:55.289744: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.290577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.291392: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.292233: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.293051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-27 15:39:55.293961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:chief/replica:0/task:0/device:GPU:0 with 752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "2025-11-27 15:39:55.305111: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:312] Creating sync eager service context with rendezvous_id on host xy /job:ps/replica:0/task:0\n",
      "2025-11-27 15:39:55.305178: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:312] Creating sync eager service context with rendezvous_id on host xy /job:ps/replica:0/task:1\n",
      "2025-11-27 15:39:55.305247: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:312] Creating sync eager service context with rendezvous_id on host xy /job:worker/replica:0/task:0\n",
      "2025-11-27 15:39:55.305321: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:312] Creating sync eager service context with rendezvous_id on host xy /job:worker/replica:0/task:1\n",
      "2025-11-27 15:39:55.305391: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:312] Creating sync eager service context with rendezvous_id on host xy /job:worker/replica:0/task:2\n",
      "2025-11-27 15:39:55.305880: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:318] SessionOptions: device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  visible_device_list: \"0\"\n",
      "  experimental {\n",
      "  }\n",
      "}\n",
      "allow_soft_placement: true\n",
      "experimental {\n",
      "  collective_group_leader: \"/job:ps/replica:0/task:0\"\n",
      "}\n",
      "\n",
      "2025-11-27 15:39:55.305891: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:318] SessionOptions: device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  visible_device_list: \"0\"\n",
      "  experimental {\n",
      "  }\n",
      "}\n",
      "allow_soft_placement: true\n",
      "experimental {\n",
      "  collective_group_leader: \"/job:ps/replica:0/task:0\"\n",
      "}\n",
      "\n",
      "2025-11-27 15:39:55.305900: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:318] SessionOptions: device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  visible_device_list: \"0\"\n",
      "  experimental {\n",
      "  }\n",
      "}\n",
      "allow_soft_placement: true\n",
      "experimental {\n",
      "  collective_group_leader: \"/job:ps/replica:0/task:0\"\n",
      "}\n",
      "\n",
      "2025-11-27 15:39:55.305913: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:318] SessionOptions: device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  visible_device_list: \"0\"\n",
      "  experimental {\n",
      "  }\n",
      "}\n",
      "allow_soft_placement: true\n",
      "experimental {\n",
      "  collective_group_leader: \"/job:ps/replica:0/task:0\"\n",
      "}\n",
      "\n",
      "2025-11-27 15:39:55.305924: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:318] SessionOptions: device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  visible_device_list: \"0\"\n",
      "  experimental {\n",
      "  }\n",
      "}\n",
      "allow_soft_placement: true\n",
      "experimental {\n",
      "  collective_group_leader: \"/job:ps/replica:0/task:0\"\n",
      "}\n",
      "\n",
      "2025-11-27 15:39:55.307728: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:53213\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:chief/replica:0/task:0/device:GPU:0'], variable_device = '/job:chief/replica:0/task:0/device:GPU:0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Number of GPUs on workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Number of GPUs on workers: 1\n"
     ]
    }
   ],
   "source": [
    "variable_partitioner = (\n",
    "    tf.distribute.experimental.partitioners.MinSizePartitioner(\n",
    "        min_shard_bytes=(256 << 10),\n",
    "        max_shards=NUM_PS))\n",
    "\n",
    "strategy = tf.distribute.ParameterServerStrategy(\n",
    "    cluster_resolver,\n",
    "    variable_partitioner=variable_partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a887ac1-3584-46fb-a9f2-ae038bfd4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 64\n",
    "\n",
    "x = tf.random.uniform((10, 10))\n",
    "y = tf.random.uniform((10,))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\n",
    "dataset = dataset.batch(global_batch_size)\n",
    "dataset = dataset.prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7be476a-a4ed-4292-833f-c0382356aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
    "\n",
    "  model.compile(tf.keras.optimizers.legacy.SGD(), loss=\"mse\", steps_per_execution=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c92ee37-f541-4ab8-b48d-94b6560f0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xy/Desktop/ml/TF/legacy/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:462: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:38.116366: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected PRODUCT element type for input 0, got type_id: TFT_ANY\n",
      "\n",
      "\twhile inferring type of node 'AutoShardDataset'\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.113981: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 1s - loss: 0.4535 - 1s/epoch - 53ms/step\n",
      "Epoch 2/5\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.169770: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.566487: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.3667 - 448ms/epoch - 22ms/step\n",
      "Epoch 3/5\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.619562: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7a869ff18f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7a869ff18f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.784571: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7a86d01317e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7a86d01317e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.3050 - 217ms/epoch - 11ms/step\n",
      "Epoch 4/5\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:39.838651: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:40.002646: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.2579 - 213ms/epoch - 11ms/step\n",
      "Epoch 5/5\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:40.054091: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:40.218827: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n",
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.2224 - 214ms/epoch - 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:40:40.269902: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:99] Ignoring error status when releasing multi-device function handle UNIMPLEMENTED: Releasing a multi-device component handle on a remote device is not yet implemented.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a86e80f87c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = \"/tmp/my_working_dir\"\n",
    "log_dir = os.path.join(working_dir, \"log\")\n",
    "ckpt_filepath = os.path.join(working_dir, \"ckpt\")\n",
    "backup_dir = os.path.join(working_dir, \"backup\")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\n",
    "    tf.keras.callbacks.BackupAndRestore(backup_dir=backup_dir),\n",
    "]\n",
    "\n",
    "model.fit(dataset, epochs=5, steps_per_epoch=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0e17c8-d475-49d4-b87c-e0b9249f485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vocab = [\n",
    "    \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\", \"wonder_woman\"\n",
    "]\n",
    "label_vocab = [\"yes\", \"no\"]\n",
    "\n",
    "with strategy.scope():\n",
    "  feature_lookup_layer = tf.keras.layers.StringLookup(\n",
    "      vocabulary=feature_vocab,\n",
    "      mask_token=None)\n",
    "  label_lookup_layer = tf.keras.layers.StringLookup(\n",
    "      vocabulary=label_vocab,\n",
    "      num_oov_indices=0,\n",
    "      mask_token=None)\n",
    "\n",
    "  raw_feature_input = tf.keras.layers.Input(\n",
    "      shape=(3,),\n",
    "      dtype=tf.string,\n",
    "      name=\"feature\")\n",
    "  feature_id_input = feature_lookup_layer(raw_feature_input)\n",
    "  feature_preprocess_stage = tf.keras.Model(\n",
    "      {\"features\": raw_feature_input},\n",
    "      feature_id_input)\n",
    "\n",
    "  raw_label_input = tf.keras.layers.Input(\n",
    "      shape=(1,),\n",
    "      dtype=tf.string,\n",
    "      name=\"label\")\n",
    "  label_id_input = label_lookup_layer(raw_label_input)\n",
    "\n",
    "  label_preprocess_stage = tf.keras.Model(\n",
    "      {\"label\": raw_label_input},\n",
    "      label_id_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296e551d-e17a-46c0-abbb-7f7e1a8ec14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_and_label_gen(num_examples=200):\n",
    "  examples = {\"features\": [], \"label\": []}\n",
    "  for _ in range(num_examples):\n",
    "    features = random.sample(feature_vocab, 3)\n",
    "    label = [\"yes\"] if \"avenger\" in features else [\"no\"]\n",
    "    examples[\"features\"].append(features)\n",
    "    examples[\"label\"].append(label)\n",
    "  return examples\n",
    "\n",
    "examples = feature_and_label_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab0a9f0-e15c-4646-98e7-a20661cfd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_fn(_):\n",
    "  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "\n",
    "  train_dataset = raw_dataset.map(\n",
    "      lambda x: (\n",
    "          {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
    "          label_preprocess_stage(x[\"label\"])\n",
    "      )).shuffle(200).batch(32).repeat()\n",
    "  return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46167a0a-8686-431f-86da-f14e9efacaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables created under the `Strategy.scope` will be placed on parameter\n",
    "# servers in a round-robin fashion.\n",
    "with strategy.scope():\n",
    "  # Create the model. The input needs to be compatible with Keras processing layers.\n",
    "  model_input = tf.keras.layers.Input(\n",
    "      shape=(3,), dtype=tf.int64, name=\"model_input\")\n",
    "\n",
    "  emb_layer = tf.keras.layers.Embedding(\n",
    "      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=16384)\n",
    "  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\n",
    "  dense_output = tf.keras.layers.Dense(\n",
    "      units=1, activation=\"sigmoid\",\n",
    "      kernel_regularizer=tf.keras.regularizers.L2(1e-4),\n",
    "  )(emb_output)\n",
    "  model = tf.keras.Model({\"features\": model_input}, dense_output)\n",
    "\n",
    "  optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)\n",
    "  accuracy = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aef629f-a6d5-4f37-8de1-5c9bb9a4ff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:ps/replica:0/task:1/device:CPU:0\n",
      "/job:ps/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "assert len(emb_layer.weights) == 2\n",
    "assert emb_layer.weights[0].shape == (4, 16384)\n",
    "assert emb_layer.weights[1].shape == (4, 16384)\n",
    "\n",
    "print(emb_layer.weights[0].device)\n",
    "print(emb_layer.weights[1].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51517491-7513-4cb6-8eaf-beb1742b832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step_fn(iterator):\n",
    "\n",
    "  def replica_fn(batch_data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(batch_data, training=True)\n",
    "      per_example_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "          reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\n",
    "      loss = tf.nn.compute_average_loss(per_example_loss)\n",
    "      model_losses = model.losses\n",
    "      if model_losses:\n",
    "        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "    accuracy.update_state(labels, actual_pred)\n",
    "    return loss\n",
    "\n",
    "  batch_data, labels = next(iterator)\n",
    "  losses = strategy.run(replica_fn, args=(batch_data, labels))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44a6c8f3-05fd-4643-be45-8708f76d79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator = tf.distribute.coordinator.ClusterCoordinator(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3309c34b-23fe-4063-b53a-3037711f8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def per_worker_dataset_fn():\n",
    "  return strategy.distribute_datasets_from_function(dataset_fn)\n",
    "\n",
    "per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\n",
    "per_worker_iterator = iter(per_worker_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69bdb8d5-c244-412c-abb3-11b147c5fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, accuracy is 0.668750.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1, accuracy is 0.487500.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2, accuracy is 0.981250.\n",
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3, accuracy is 1.000000.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "steps_per_epoch = 5\n",
    "for i in range(num_epochs):\n",
    "  accuracy.reset_states()\n",
    "  for _ in range(steps_per_epoch):\n",
    "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
    "  # Wait at epoch boundaries.\n",
    "  coordinator.join()\n",
    "  print(\"Finished epoch %d, accuracy is %f.\" % (i, accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eef4d420-7051-4cfc-9738-55e58b7a0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss is 0.242300\n"
     ]
    }
   ],
   "source": [
    "loss = coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
    "print(\"Final loss is %f\" % loss.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b93c9790-450b-44d2-b2e6-dcb761f4d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    feature_and_label_gen(num_examples=16)).map(\n",
    "          lambda x: (\n",
    "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
    "              label_preprocess_stage(x[\"label\"])\n",
    "          )).batch(8)\n",
    "\n",
    "eval_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for batch_data, labels in eval_dataset:\n",
    "  pred = model(batch_data, training=False)\n",
    "  actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "  eval_accuracy.update_state(labels, actual_pred)\n",
    "\n",
    "print(\"Evaluation accuracy: %f\" % eval_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94cfd649-3186-4bb0-a499-a441ae94282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:1 GPUs are allocated per worker. Please use DistributedDataset by calling strategy.experimental_distribute_dataset or strategy.distribute_datasets_from_function to make best use of GPU resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for all global closures to be finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  # Define the eval metric on parameter servers.\n",
    "  eval_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "@tf.function\n",
    "def eval_step(iterator):\n",
    "  def replica_fn(batch_data, labels):\n",
    "    pred = model(batch_data, training=False)\n",
    "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "    eval_accuracy.update_state(labels, actual_pred)\n",
    "  batch_data, labels = next(iterator)\n",
    "  strategy.run(replica_fn, args=(batch_data, labels))\n",
    "\n",
    "def eval_dataset_fn():\n",
    "  return tf.data.Dataset.from_tensor_slices(\n",
    "      feature_and_label_gen(num_examples=16)).map(\n",
    "          lambda x: (\n",
    "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
    "              label_preprocess_stage(x[\"label\"])\n",
    "          )).shuffle(16).repeat().batch(8)\n",
    "\n",
    "per_worker_eval_dataset = coordinator.create_per_worker_dataset(eval_dataset_fn)\n",
    "per_worker_eval_iterator = iter(per_worker_eval_dataset)\n",
    "\n",
    "eval_steps_per_epoch = 2\n",
    "for _ in range(eval_steps_per_epoch):\n",
    "  coordinator.schedule(eval_step, args=(per_worker_eval_iterator,))\n",
    "coordinator.join()\n",
    "print(\"Evaluation accuracy: %f\" % eval_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdb32f-a24e-4ddb-a617-61234374c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e3569-8d30-493c-a302-2029c9f9295e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f51f3-d0fa-451b-827f-2848eff82bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073f885-b70d-4203-a877-c4ef4f46702a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e333967-4df9-4e88-b0b3-fb2b68a59fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf81e-ec42-4104-bb12-3ac00c1e8322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d014-39d3-421f-a2cf-e1f0424756ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419a001-ca6c-429d-94ad-0b1eaf11d539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df4ab1-fe5d-4269-8872-bdc8d928fca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14882776-71cd-4be9-b1d7-040e9dd345cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebcff2-2612-4fa0-8280-d0813af4e436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aab030-70d8-4333-831d-363654e190ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101b3f8-6665-496c-9ae0-f405b2918c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cb255-beb0-483e-bf4d-9dc1b47e70e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ffad27-11f5-419f-bdb9-4af1fafde8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
