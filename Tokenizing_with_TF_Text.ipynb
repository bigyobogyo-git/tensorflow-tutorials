{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160fb1b5-777a-4743-a1f2-af3eba08fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 16:58:45.781938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764086325.793893   17883 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764086325.797293   17883 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764086325.808606   17883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764086325.808616   17883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764086325.808617   17883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764086325.808619   17883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-25 16:58:45.811755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656745ca-a2e3-4126-94a7-10f095daeb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764086363.121534   17883 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 554 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5f90b3-0a68-46cd-862a-50fba32a8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b'can', b\"'\", b't', b'explain', b',', b'but', b'you', b'feel', b'it', b'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9298bb9c-46db-4e54-9569-7ab23acca19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "266fd214-5ea5-4f78-82cb-6de54d3c083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
    "r = requests.get(url)\n",
    "filepath = \"vocab.txt\"\n",
    "open(filepath, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6711b01-7198-493c-ae61-9fc35aac6588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[b'What'], [b'you'], [b'know'], [b'you'], [b\"can't\"], [b'explain,'], [b'but'], [b'you'], [b'feel'], [b'it.']]]\n"
     ]
    }
   ],
   "source": [
    "subtokenizer = tf_text.UnicodeScriptTokenizer(filepath)\n",
    "subtokens = tokenizer.tokenize(tokens)\n",
    "print(subtokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61fd27f-d560-4f29-9a0b-50d5e13f8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[b'what'], [b'you'], [b'know'], [b'you'], [b'can'], [b\"'\"], [b't'], [b'explain'], [b','], [b'but'], [b'you'], [b'feel'], [b'it'], [b'.']]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.BertTokenizer(filepath, token_out_type=tf.string, lower_case=True)\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625f1d02-1d41-4faf-b1c7-d5d4e8d56a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
    "sp_model = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de8ea8e-8545-48f1-b1ea-527a05057188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe2\\x96\\x81What', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81know', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81can', b\"'\", b't', b'\\xe2\\x96\\x81explain', b',', b'\\xe2\\x96\\x81but', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81feel', b'\\xe2\\x96\\x81it', b'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.SentencepieceTokenizer(sp_model, out_type=tf.string)\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5186d3-9792-4e08-9ff3-ebcc3c50cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea8a491-3f9d-4d44-8e10-fcfc979ecaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Wh', b'ha', b'at', b't ', b' y', b'yo', b'ou', b'u ', b' k', b'kn', b'no', b'ow', b'w ', b' y', b'yo', b'ou', b'u ', b' c', b'ca', b'an', b\"n'\", b\"'t\", b't ', b' e', b'ex', b'xp', b'pl', b'la', b'ai', b'in', b'n,', b', ', b' b', b'bu', b'ut', b't ', b' y', b'yo', b'ou', b'u ', b' f', b'fe', b'ee', b'el', b'l ', b' i', b'it', b't.']]\n"
     ]
    }
   ],
   "source": [
    "characters = tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")\n",
    "bigrams = tf_text.ngrams(characters, 2, reduction_type=tf_text.Reduction.STRING_JOIN, string_separator='')\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd76d4bc-102a-4146-a92d-c71ef32c9546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xy/Desktop/ml/TF/tf_2.19.1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading 4 files:   0%|                                | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/zh-segmentation/tensorFlow1/zh-segmentation/1/download/saved_model.pb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                               | 0.00/1.51M [00:00<?, ?B/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/zh-segmentation/tensorFlow1/zh-segmentation/1/download/tfhub_module.pb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 2.00/2.00 [00:00<00:00, 6.01kB/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/zh-segmentation/tensorFlow1/zh-segmentation/1/download/variables/variables.index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 331/331 [00:00<00:00, 2.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▏            | 1.00M/1.51M [00:00<00:00, 2.01MB/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/zh-segmentation/tensorFlow1/zh-segmentation/1/download/variables/variables.data-00000-of-00001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1.51M/1.51M [00:00<00:00, 2.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|████████████▍                         | 1.00M/3.04M [00:00<00:01, 1.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 3.04M/3.04M [00:00<00:00, 4.76MB/s]\u001b[A\u001b[A\n",
      "Downloading 4 files: 100%|████████████████████████| 4/4 [00:06<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /home/xy/.cache/kagglehub/models/google/zh-segmentation/tensorFlow1/zh-segmentation/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/zh-segmentation/tensorFlow1/zh-segmentation\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d466476-727d-430b-a210-4b3346616988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764086733.125195   18084 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe6\\x96\\xb0\\xe5\\x8d\\x8e\\xe7\\xa4\\xbe', b'\\xe5\\x8c\\x97\\xe4\\xba\\xac']]\n"
     ]
    }
   ],
   "source": [
    "MODEL_HANDLE = path\n",
    "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
    "tokens = segmenter.tokenize([\"新华社北京\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a97a46c-648e-445a-ba2d-f5e59bc49b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京']]\n"
     ]
    }
   ],
   "source": [
    "def decode_list(x):\n",
    "  if type(x) is list:\n",
    "    return list(map(decode_list, x))\n",
    "  return x.decode(\"UTF-8\")\n",
    "\n",
    "def decode_utf8_tensor(x):\n",
    "  return list(map(decode_list, x.to_list()))\n",
    "\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "474e3200-d7c1-4aff-b7c5-598f9f6592db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京']]\n"
     ]
    }
   ],
   "source": [
    "strings = [\"新华社北京\"]\n",
    "labels = [[0, 1, 1, 0, 1]]\n",
    "tokenizer = tf_text.SplitMergeTokenizer()\n",
    "tokens = tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db84ba6c-8c83-4c6b-ae3b-b3cdf0e8ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京']]\n"
     ]
    }
   ],
   "source": [
    "strings = [[\"新华社北京\"]]\n",
    "labels = [[[5.0, -3.2], [0.2, 12.0], [0.0, 11.0], [2.2, -1.0], [-3.0, 3.0]]]\n",
    "tokenizer = tf_text.SplitMergeFromLogitsTokenizer()\n",
    "tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c9a26ef-61ab-4482-b860-2062a4970259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "splitter = tf_text.RegexSplitter(\"\\s\")\n",
    "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae585e3-cde7-44e6-b634-f3e9fcc6772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.']]\n",
      "[[0, 11, 15, 21, 26, 29, 33]]\n",
      "[[10, 14, 20, 25, 28, 33, 34]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
    "print(tokens.to_list())\n",
    "print(start_offsets.to_list())\n",
    "print(end_offsets.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29e4bb65-49a6-4a9e-b382-a6bfa5bb8053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n",
      "[b\"What you know you can't explain, but you feel it.\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())\n",
    "strings = tokenizer.detokenize(tokens)\n",
    "print(strings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87ff85cd-6093-47b6-8f63-18fb3a70c49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c1b6d-66c5-4606-94a9-e60041db6c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c2987-dcd0-4d0c-9363-6fd79b884b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2082d1-10f7-42ed-be7b-c325b548058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01861abd-cd4c-4e55-96b9-ce4f4d4f87e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6dcb5-c237-485d-b49d-8b1aa177bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e3aae-ff08-4de6-891e-6e3b6f8eeefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83defe79-6e82-47a6-9ebd-05ea5310cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaecd2-af87-41f8-9927-a1985ef8175a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
